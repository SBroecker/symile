# Symile-M3
<img src="/img/symile_m3.png" alt="Symile-M3" width="800"/>

In this section, we describe how to generate Symile-M3, a new multilingual dataset of 33 million (audio, image, text) samples. The dataset is specifically designed to test a model's ability to capture higher-order information between three distinct high-dimensional data types: by incorporating multiple languages, we construct a task where text and audio are both needed to predict the image, and where, importantly, neither text nor audio alone would suffice. See our [paper](https://arxiv.org/abs/2411.01053) for details on the dataset.

Download the dataset from Hugging Face [here](https://huggingface.co/datasets/arsaporta/symile-m3).

## Generate data

### Setup

#### Download ImageNet and Common Voice data

Symile-M3 is generated by sampling images from ImageNet and audio clips from Common Voice, which you will need to download:
- **ImageNet:** To download the ImageNet data, sign into Kaggle and download the data [here](https://www.kaggle.com/c/imagenet-object-localization-challenge/data?select=ILSVRC).
- **Common Voice:** You can download the Common Voice data [here](https://commonvoice.mozilla.org/en/datasets).

#### Google Cloud API

You will need the Google Cloud Translation client library. Follow the instructions [here](https://cloud.google.com/translate/docs/setup) to create a project that has the Cloud Translation API enabled and credentials to make authenticated calls. When you get to the section [Installing client libraries](https://cloud.google.com/translate/docs/setup#installing_client_libraries), you will only need the basic client libraries.

#### Activate environment

To start, make sure you're in the correct directory:
```
> cd experiments/data_processing/symile_m3
```

##### conda

```
> conda env create -f env/environment-generate.yml
> conda activate symile-m3-generate
(symile-m3-generate) >
```

##### pip

```
> python -m venv venv
> source venv/bin/activate
(venv) > pip install -r env/requirements-generate.txt
```

### Generate translations

The `generate_translations.py` script creates a JSON file (`args.translations_path`) that contains translations for ImageNet class names in multiple languages. This file maps each ImageNet class to its translated names. The script uses the Google Translate API for automatic translations.

The format of the JSON file (`args.translations_path`) is as follows:

```
{
    "tench": {
        "synset_id": "n01440764",
        "ar": "التنش سمك نهري",
        "el": "είδος κυπρίνου",
        "en": "tench",
        "cls_id": 0
    },
    "goldfish": {
        "synset_id": "n01443537",
        "ar": "سمكة ذهبية",
        "el": "χρυσόψαρο",
        "en": "goldfish",
        "cls_id": 1
    }
}
```

Since automatic translations can sometimes be incorrect or identical across different languages, the script also supports manual adjustments using a separate JSON file (`args.manual_translations_path`). This file allows you to correct translations or avoid overlap between languages. The format for the manual translations file is as follows:
```
{
    "abacus": {
        "synset_id": "n02666196",
        "ur": "گنتی کا فریم"
    },
    "abaya": {
        "synset_id": "n02667093",
        "el": "φόρεμα abaya"
    }
}
```

To run the script, use the following command:
```
(symile-m3-generate) > python generate_translations.py \
    --translations_path /path/to/translations.json \
    --imagenet_classmapping_path /path/to/LOC_synset_mapping.txt \
    --manual_translations_path /path/to/manual_translations.json
```
where `LOC_synset_mapping.txt` is a file provided by ImageNet that maps each synset ID to its class description.

### Filter empty audio clips
Symile-M3 is generated by sampling audio clips from the Common Voice dataset. However, some of these audio clips have a duration of 0.0 seconds, which this script removes.

To run the script, use the following command:
```
(symile-m3) > python cv_processing.py
```

### Generate dataset splits

This script creates CSV files for the training, validation, and test splits used in Symile-M3. The script generates text by sampling audio clips and images, then creating multilingual text from the corresponding translations. It also allows the user to specify whether to allow overlap across languages and classes or to keep them disjoint.

Symile-M3-2 was created with `data_type = disjoint`, `num_words = 2`, and `num_langs = 2`. Symile-M3-5 was created with `data_type = disjoint`, `num_words = 5`, and `num_langs = 5`. Symile-M3-10 was created with `data_type = disjoint`, `num_words = 10`, and `num_langs = 10`.

```
(symile-m3-generate) > python generate_data.py \
    --train_n 10000000 \
    --val_n 500000 \
    --test_n 500000 \
    --data_type disjoint \
    --num_words 10 \
    --num_langs 10 \
    --translations_path /path/to/translations.json \
    --save_dir /path/to/save_dir
```
## Preprocess data

### Activate environment

To start, make sure you're in the correct directory:
```
> cd experiments/data_processing/symile_m3
```

##### conda

```
> conda env create -f env/environment-preprocess.yml
> conda activate symile-m3-preprocess
(symile-m3-preprocess) >
```

##### pip

```
> python -m venv venv
> source venv/bin/activate
(venv) > pip install -r env/requirements-preprocess.txt
```

### Preprocess and save dataset tensors

To accelerate training, we can loads and preprocesses the Symile-M3 dataset splits, saving the resulting tensors to split-specific directories.

First, we calculate the maximum token length for the text column across the train, validation, and test splits. This script uses a tokenizer (e.g., BERT, XLM-RoBERTa, or mT5) to tokenize the text from each split and determine the maximum token length for each dataset. The maximum lengths are then saved to a JSON file for later use in preprocessing.

```
(symile-m3-preprocess) > python max_token_len.py \
    --text_model_id <Hugging Face model id for text tokenizer, e.g. xlm-roberta-large> \
    --data_dir <path to directory where train.csv, val.csv, and test.csv are saved> \
    --save_pt /path/to/max_token_len.json
```

Next, we load and preprocess the Symile-M3 dataset splits (train, validation, and test), saving the resulting tensors to split-specific directories. Audio and image representations (the output from their respective encoders) are saved because the audio and image encoders are not fine-tuned in our experiments. Since the text encoder is fine-tuned, only the text tokens are saved.

```
(symile-m3-preprocess) > python save_representations.py \
    --audio_model_id <Hugging Face model id for audio, e.g. openai/whisper-large-v3> \
    --image_model_id <Hugging Face model id for audio, e.g. openai/clip-vit-large-patch14> \
    --text_model_id <Hugging Face model id for text tokenizer, e.g. xlm-roberta-large> \
    --batch_sz_train 128 \
    --batch_sz_val 128 \
    --batch_sz_test 128 \
    --drop_last False \
    --data_dir <path to directory where train.csv, val.csv, and test.csv are saved> \
    --save_dir /path/to/save_dir
    --max_token_len_pt /path/to/max_token_len.json \
    --split_to_run <split to process e.g. train, val, test>
```

#### Handling large datasets
Since the `train.csv` file can be quite large, the following steps allow for parallel processing by splitting the dataset into smaller sub-dataframes, processing each individually, and then merging the results back together.

After running `max_token_len.py`, use the `split_df.py` script to split the large `train.csv` file into smaller sub-CSV files (e.g., `train0.csv`, `train1.csv`, etc.). This allows `save_representations.py` to be run in parallel across multiple sub-dataframes.

```
python split_df.py \
    --csv_to_split /path/to/train.csv \
    --sub_df_size 500000 \
    --save_dir /path/to/save_sub_csvs
```

After splitting the dataset, run `save_representations.py` on each sub-CSV file. You can run these processes in parallel to accelerate the preprocessing. Use the `train_csv` flag to specify the sub-CSV file to process:
```
(symile-m3-preprocess) > python save_representations.py \
    --audio_model_id <Hugging Face model id for audio, e.g. openai/whisper-large-v3> \
    --image_model_id <Hugging Face model id for audio, e.g. openai/clip-vit-large-patch14> \
    --text_model_id <Hugging Face model id for text tokenizer, e.g. xlm-roberta-large> \
    --batch_sz_train 128 \
    --batch_sz_val 128 \
    --batch_sz_test 128 \
    --drop_last False \
    --data_dir <path to directory where train.csv, val.csv, and test.csv are saved> \
    --save_dir /path/to/save_dir
    --max_token_len_pt /path/to/max_token_len.json \
    --train_csv train0.csv \
    --split_to_run train
```

Once the representations for all sub-dataframes have been processed, use the `merge_representations.py` script to combine the results into a single set of tensors. This script merges the representations (audio, image, text, and other metadata) that were saved after processing each sub-CSV file. It assumes that the `data_dir` contains subdirectories, each corresponding to one of the sub-CSV files. The subdirectories are expected to be named sequentially (e.g., train0, train1, train2, etc.) based on the number of sub-CSV splits. Each subdirectory must contain the relevant .pt and .txt files, named according to the format `<type>_train{i}`, where `{i}` is the subdirectory number (these files and subdirectories will be created automatically when you run the `save_representations.py` script for each sub-CSV file).

```
(symile-m3-preprocess) > python merge_representations.py \
    --data_dir /path/to/subdirectories_with_representations \
    --save_dir /path/to/final_merged_representations \
    --num_subdirs 20
```

The `num_subdirs` flag specifies the number of subdirectories that the script will loop through to find and merge the individual representation files. For example, if you split your dataset into 20 sub-CSV files, you should set num_subdirs=20, and the script will expect subdirectories named `train0` through `train19`.

This script will then load the .pt files from each subdirectory, concatenate the tensors from all subdirectories, and save the merged tensors to a final directory (`save_dir`).

#### Missingness

To generate versions of Symile-M3 with missing data, use the `add_missingness.py` script. This script creates missingness indicators by generating and saving a tensor for each modality (text, image, audio) according to a specified probability. Each indicator tensor is drawn from a Bernoulli distribution, with each element set to 1 (indicating missing data) with probability `missingness_prob`, and 0 otherwise.

```
(symile-m3-preprocess) > python add_missingness.py \
    --data_dir <path to directory where split-specific idx_<split>.pt files are saved> \
    --save_dir <path to directory where missingness tensors will be saved> \
    --missingness_prob <probability of missingness for each modality, e.g., 0.5>
```
